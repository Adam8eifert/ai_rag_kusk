# AI RAG pipeline â€“ dotazovÃ¡nÃ­ nad PDF dokumenty (bez halucinacÃ­)

Tento projekt implementuje **robustnÃ­ RAG (Retrievalâ€‘Augmented Generation) systÃ©m** nad **lokÃ¡lnÃ­mi PDF dokumenty** (napÅ™. smlouvami, internÃ­ dokumentacÃ­).

HlavnÃ­m cÃ­lem je:

* pracovat vÃ½hradnÄ› s uÅ¾ivatelem dodanÃ½mi dokumenty,
* zabrÃ¡nit halucinacÃ­m,
* vracet pouze odpovÄ›di podloÅ¾enÃ© zdroji s citacemi,
* umoÅ¾nit bezpeÄnÃ© pouÅ¾itÃ­ v â€enterprise" prostÅ™edÃ­,
* **odpovÃ­dat vÃ½hradnÄ› v ÄeÅ¡tinÄ›** (bez pÅ™ekladu, v jazyce dokumentu).

Projekt je **hybridnÃ­**: lze jej pouÅ¾Ã­vat jako:
* **Pure Extractive RAG** (bez LLM, deterministic, nejbezpeÄnÄ›jÅ¡Ã­),
* **RAG s LLM syntÃ©zou** (LLM zkrÃ¡tÃ­/pÅ™eformuluje text, ale nikdy si nic nedomÃ½Å¡lÃ­).

---

## Co tento projekt dÄ›lÃ¡

* naÄte PDF dokumenty z lokÃ¡lnÃ­ sloÅ¾ky `data/`,
* extrahuje text (strÃ¡nku po strÃ¡nce),
* rozdÄ›lÃ­ jej na overlapping chunky (sliding window),
* vytvoÅ™Ã­ embeddingy pomocÃ­ Sentence Transformers (multilingual),
* uloÅ¾Ã­ vektorovÃ½ index (FAISS IndexFlatIP),
* umoÅ¾nÃ­ dotazovÃ¡nÃ­ pÅ™es REST API (FastAPI),
* **odpovÃ­dÃ¡ vÃ½hradnÄ› z obsahu dokumentÅ¯** (bez domÃ½Å¡lenÃ­),
* **cituje source** (soubor, strÃ¡nka, chunk_id) + confidence score.

---

## ğŸ›¡ï¸ StriktnÃ­ pravidla bez halucinacÃ­ (Compliance-friendly)

Projekt implementuje **tÅ™i vrstvy ochrany** proti halucinacÃ­m a spekulacÃ­m:

### 1ï¸âƒ£ Hard Factual Gate (threshold 0.72)

```
if top_score < 0.72:
    âŒ LLM se NESMÃ zavolat
    âœ… VrÃ¡t fallback odpovÄ›Ä
```

**Princip:** Pokud nejrelevantnÄ›jÅ¡Ã­ chunk mÃ¡ cosine similarity < 0.72 (tj. skÃ³re < 72%), vracÃ­ se okamÅ¾itÄ› fallback zprÃ¡va. LLM se volÃ¡ **jen pokud** score >= 0.72.

```python
{
  "answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech.",
  "sources": [],
  "confidence": 0.68
}
```

### 2ï¸âƒ£ Keyword Guard (relevance check)

PÅ™ed zavolÃ¡nÃ­m LLM se ovÄ›Å™Ã­, Å¾e:
- OtÃ¡zka obsahuje alespoÅˆ 2 klÃ­ÄovÃ¡ slova
- Kontext obsahuje alespoÅˆ 2 z tÄ›chto slov
- Pokud ne â†’ fallback (otÃ¡zka je mimo scope dokumentu)

**PÅ™Ã­klady:**
- âœ… "JakÃ½ je doba plnÄ›nÃ­?" + kontext s "doba" + "plnÄ›nÃ­" = OK
- âŒ "Kdo je skuteÄnÃ½ vlastnÃ­k?" + smlouva bez "vlastnÃ­k" = Fallback
- âŒ "JakÃ© riziko smlouva pÅ™edstavuje?" (evaluaÄnÃ­, ne faktickÃ¡) = Fallback

### 3ï¸âƒ£ LLM Compression Mode (ne generÃ¡tor)

LLM (pokud je zapnutÃ½) je **JEN kompresor**:
- âœ… ZkrÃ¡tÃ­ text na max 3 vÄ›ty
- âœ… ZachovÃ¡vÃ¡ faktickÃ© znÄ›nÃ­
- âŒ NesmÃ­ generovat novÃ© informace
- âŒ NesmÃ­ odpovÃ­dat sÃ¡m bez kontextu

**System prompt:**
```
"NepÅ™idÃ¡vej Å¾Ã¡dnÃ© novÃ© informace"
"OdpovÃ­dej POUZE z poskytnutÃ©ho textu"
"Pokud nejsi si jistÃ½, radÄ›ji vynech"
```

---

## Architektura

```
PDF (data/)
  â†“
Extrakce textu (pdfplumber) - strÃ¡nka po strÃ¡nce
  â†“
Chunking (sliding window: chunk_size=200 slov, overlap=50)
  â†“
Embeddingy (SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2)
  â†“
Normalizace vektorÅ¯ (L2 norm = jednotkovÃ¡ dÃ©lka)
  â†“
FAISS Index (IndexFlatIP: vnitÅ™nÃ­ souÄin = kosinusovÃ¡ podobnost)
  â†“
Semantic retrieval (top-k chunky seÅ™azenÃ© podle similarity)
  â†“
â”Œâ”€ Extractive mode (use_llm=False) â†’ syrovÃ© chunky
â””â”€ LLM Synthesis (use_llm=True) â†’ GPT-3.5 (OpenAI) nebo FLAN-T5 (offline)
  â†“
[SYSTEM_PROMPT: "OdpovÃ­dej POUZE z kontextu, bez domÃ½Å¡lenÃ­"]
  â†“
OdpovÄ›Ä + sources + confidence score
  â†“
FastAPI /ask endpoint
  â†“
Logging do queries.jsonl (audit trail)
```

---

## Struktura projektu

```
.
â”œâ”€â”€ data/               # VstupnÃ­ PDF dokumenty (sem vklÃ¡dej smlouvy atd.)
â”œâ”€â”€ index/              # FAISS index + metadata (automaticky generovanÃ©)
â”œâ”€â”€ logs/               # queries.jsonl (audit trail vÅ¡ech dotazÅ¯)
â”‚
â”œâ”€â”€ build_index.py      # Indexace: PDF â†’ chunky â†’ embeddingy â†’ FAISS
â”œâ”€â”€ rag.py              # RAGEngine (chunking, embedding, retrieval, synthesis)
â”œâ”€â”€ app.py              # FastAPI aplikace s /ask endpointem
â”œâ”€â”€ llm.py              # LLM wrapper (OpenAI API / FLAN-T5 fallback)
â”œâ”€â”€ pdf_loader.py       # PDF text extraction (pdfplumber)
â”œâ”€â”€ rules.py            # Optional: question type rules
â”‚
â”œâ”€â”€ requirements.txt    # Pip dependencies
â””â”€â”€ README.md           # Tato dokumentace
```

---

## Popis hlavnÃ­ch souborÅ¯

### `build_index.py` â€“ VytvoÅ™enÃ­ FAISS indexu

```bash
python build_index.py
```

**Procedura:**
1. Najde vÅ¡echny `*.pdf` soubory v `data/`
2. Extrahuje text (pdfplumber, per page)
3. RozdÄ›lÃ­ na chunky (sliding window: chunk_size=200 slov, overlap=50)
4. VypoÄÃ­tÃ¡ embeddingy (SentenceTransformer) po dÃ¡vkÃ¡ch
5. Normalizuje vektory (vyÅ¾aduje IndexFlatIP)
6. VytvoÅ™Ã­ FAISS index

**VÃ½stup:**
```
index/
â”œâ”€â”€ faiss.index         # VektorovÃ½ index (binÃ¡rnÃ­, IndexFlatIP)
â””â”€â”€ documents.json      # Metadata: file, page, chunk_id, text
```

> PÅ™i zmÄ›nÄ› dokumentÅ¯ v `data/` je nutnÃ© znovu spustit `build_index.py`.

---

### `rag.py` â€“ RAGEngine (jÃ¡dro systÃ©mu)

TÅ™Ã­da `RAGEngine` zajiÅ¡Å¥uje:

* **chunking** (sliding window s overlappem),
* **embedding** (SentenceTransformer encoder, 384-dim),
* **retrieval** (FAISS semantic search, cosine similarity),
* **answer synthesis** (extractive nebo LLM-based),
* **confidence scoring** (avg similarity Retrieved chunks).

#### Dva mÃ³dy odpovÃ­dÃ¡nÃ­

**MÃ³d 1: Extractive** (`use_llm=False`)
* VracÃ­ raw text z nejrelevantnÄ›jÅ¡Ã­ch chunkÅ¯
* DeterministickÃ©, bez LLM
* Fastest, nÃ­zkÃ¡ latence
* IdeÃ¡lnÃ­ pro "fully auditable" answers

**MÃ³d 2: LLM Synthesis** (`use_llm=True`)
* **Priorita:** OpenAI API (GPT-3.5-turbo, pokud je `OPENAI_API_KEY`)
* **Fallback:** MÃ­stnÃ­ FLAN-T5 (offline, bez API klÃ­Äe)
* LLM **zkrÃ¡tÃ­/pÅ™eformuluje** text ale **vÅ¾dy jen z kontextu**
* System prompt: "OdpovÃ­dej POUZE z poskytnutÃ©ho kontextu, bez domÃ½Å¡lenÃ­"

##### SYSTEM_PROMPT â€“ Srdce bezpeÄnosti

```python
SYSTEM_PROMPT = """
Jsi extrakÄnÃ­ asistent pro analÃ½zu smluvnÃ­ch dokumentÅ¯.

OdpovÃ­dej vÃ½hradnÄ› v ÄeskÃ©m jazyce.

OdpovÃ­dej POUZE na zÃ¡kladÄ› poskytnutÃ©ho kontextu.
Nic si nedomÃ½Å¡lej, neodvozuj a nepÅ™idÃ¡vej.

Pokud odpovÄ›Ä nelze jednoznaÄnÄ› najÃ­t v kontextu,
odpovÄ›z pÅ™esnÄ› touto vÄ›tou:
"PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech explicitnÄ› uvedena."
"""
```

Tento prompt se **automaticky** pÅ™edÃ¡vÃ¡:
* **OpenAI API:** `messages = [{"role": "system", "content": SYSTEM_PROMPT}, ...]`
* **FLAN-T5:** `prompt = "{SYSTEM_PROMPT}\n\nKONTEXT:\n{context}\n\nOTÃZKA:\n{question}"`

---

### `app.py` â€“ FastAPI aplikace

JedinÃ½ endpoint:

```
POST /ask
```

#### Request

```json
{
  "question": "JakÃ¡ je doba plnÄ›nÃ­?",
  "strict": false,
  "k": 5,
  "use_llm": true
}
```

**Parametry:**
* `question` *(required)* â€“ Dotaz v ÄeÅ¡tinÄ›
* `strict` *(optional, default=False)* â€“ Pokud True: niÅ¾Å¡Ã­ prÃ¡h, kratÅ¡Ã­ odpovÄ›di (2-3 vÄ›ty)
* `k` *(optional, default=5)* â€“ PoÄet chunkÅ¯ k naÄtenÃ­ z FAISS
* `use_llm` *(optional, default=True)* â€“ Pokud True: LLM synthesis; pokud False: raw text

#### Response

```json
{
  "answer": "Doba plnÄ›nÃ­ je 30 dnÃ­ od podpisu smlouvy.",
  "sources": [
    {
      "file": "smlouva_ABC.pdf",
      "page": 2,
      "chunk_id": 5
    }
  ],
  "confidence": 0.87
}
```

**Pole:**
* `answer` â€“ OdpovÄ›Ä na otÃ¡zku (Czech)
* `sources` â€“ List zdrojÅ¯ (file, page, chunk_id)
* `confidence` â€“ Cosine similarity (0.0â€“1.0)

---

### `llm.py` â€“ LLM wrapper (hibridnÃ­ design)

```python
from llm import LLMWrapper

llm = LLMWrapper(use_openai=True)  # Nebo False
answer = llm.synthesize(question, context)
```

**Inicializace:**
1. ZkusÃ­ naÄÃ­st `OPENAI_API_KEY` environment variable
2. Pokud existuje: inicializuje `openai.OpenAI()` client â†’ **OpenAI mode**
3. Pokud ne: fallback na `google/flan-t5-base` â†’ **FLAN-T5 mode** (offline)
4. Pokud ani to nenÃ­ dostupnÃ©: vracÃ­ syrovÃ½ kontext

**SystÃ©movÃ½ prompt** je konsistentnÃ­ v obou mode (viz `SYSTEM_PROMPT` vÃ½Å¡e).

---

### `pdf_loader.py` â€“ PDF text extraction

```python
from pdf_loader import load_pdf

pages = load_pdf("data/smlouva.pdf")
# VrÃ¡tÃ­: [{"text": "...", "page": 1, "source": "..."}, ...]
```

Extrahuje text strÃ¡nku po strÃ¡nce. Pokud je PDF obrÃ¡zek bez OCR, vrÃ¡tÃ­ `text=""`.

---

### `rules.py` â€“ Optional question type filtering

```python
QUESTION_RULES = {
    "doba_plneni": {
        "question_keywords": ["doba plnÄ›nÃ­", "termÃ­n plnÄ›nÃ­"],
        "section_keywords": ["doba plnÄ›nÃ­", "plnÄ›nÃ­ smlouvy"],
        "min_similarity": 0.75
    },
    ...
}
```

PouÅ¾Ã­vÃ¡ se v `rag.answer_question()` (extractive mode s tvrdÃ½mi pravidly).

---

## Instalace a spuÅ¡tÄ›nÃ­

### 1ï¸âƒ£ VytvoÅ™enÃ­ virtuÃ¡lnÃ­ho prostÅ™edÃ­

```bash
python -m venv .venv
source .venv/bin/activate      # Linux/Mac
# nebo
.venv\Scripts\activate         # Windows
```

### 2ï¸âƒ£ Instalace zÃ¡vislostÃ­

```bash
pip install -r requirements.txt
```

Obsah `requirements.txt`:
```
pdfplumber~=0.10
sentence-transformers~=2.2
faiss-cpu~=1.8
numpy~=1.23
fastapi~=0.104
uvicorn~=0.24
```

**OpenAI API (optional):**
```bash
pip install openai~=1.0
```

Bez OpenAI balÃ­Äku se projekt automaticky pÅ™epne na FLAN-T5 (offline).

### 3ï¸âƒ£ PÅ™Ã­prava dokumentÅ¯

VloÅ¾te PDF soubory do `data/`:

```
data/
â”œâ”€â”€ smlouva_ABC.pdf
â”œâ”€â”€ smlouva_DEF.pdf
â””â”€â”€ interni_pravidla.pdf
```

### 4ï¸âƒ£ Indexace dokumentÅ¯

```bash
python build_index.py
```

OÄekÃ¡vanÃ½ vÃ½stup:
```
ğŸš€ Inicializace RAGEngine...
ğŸ“ BudovÃ¡nÃ­ indexu z PDF souborÅ¯ v 'data/' adresÃ¡Å™i...
ğŸ“„ ÄŒtenÃ­: smlouva_ABC.pdf
ğŸ“„ ÄŒtenÃ­: smlouva_DEF.pdf
âœ“ SbÃ­rÃ¡no 156 chunkÅ¯
ğŸ”„ EnkÃ³dovÃ¡nÃ­ 156 textÅ¯ po dÃ¡vkÃ¡ch x64...
âœ“ Embeddingy shape: (156, 384)
âœ“ Vektory normalizovÃ¡ny
âœ“ FAISS IndexFlatIP vytvoÅ™en (dim=384, n=156)
âœ… Index uloÅ¾en: index/faiss.index
âœ… Metadata uloÅ¾ena: index/documents.json
```

### 5ï¸âƒ£ SpuÅ¡tÄ›nÃ­ API

```bash
uvicorn app:app --reload
```

Nebo s customnÃ­m hostem/portem:
```bash
uvicorn app:app --host 0.0.0.0 --port 8000
```

API dostupnÃ© na:
* **API root:** http://127.0.0.1:8000
* **Swagger UI:** http://127.0.0.1:8000/docs â† **Zde testuj**
* **ReDoc:** http://127.0.0.1:8000/redoc

---

## PraktickÃ© pÅ™Ã­klady

### PÅ™Ã­klad 1: Extractive mode (bez LLM, pure deterministic)

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "JakÃ¡ je doba plnÄ›nÃ­?",
    "use_llm": false
  }'
```

**OdpovÄ›Ä:**
```json
{
  "answer": "Doba plnÄ›nÃ­ je 30 dnÃ­ od podpisu smlouvy. PlnÄ›nÃ­ se musÃ­ uskuteÄnit v pracovnÃ­ch dnech. Dodavatel je povinen dodrÅ¾ovat dohodnutÃ© termÃ­ny.",
  "sources": [
    {"file": "smlouva_ABC.pdf", "page": 2, "chunk_id": 3}
  ],
  "confidence": 0.92
}
```

### PÅ™Ã­klad 2: LLM synthesis (zkrÃ¡cenÃ¡, pÅ™eformulovanÃ¡ odpovÄ›Ä)

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "JakÃ¡ je doba plnÄ›nÃ­?",
    "use_llm": true,
    "strict": true
  }'
```

**OdpovÄ›Ä (zkrÃ¡cenÃ¡ OpenAI/FLAN-T5):**
```json
{
  "answer": "Doba plnÄ›nÃ­ smlouvy je 30 dnÃ­ od podpisu.",
  "sources": [
    {"file": "smlouva_ABC.pdf", "page": 2, "chunk_id": 3}
  ],
  "confidence": 0.92
}
```

### PÅ™Ã­klad 3: Swagger UI â€“ InteraktivnÃ­ testovÃ¡nÃ­

1. OtevÅ™i v prohlÃ­Å¾eÄi: **http://127.0.0.1:8000/docs**
2. Klikni na `POST /ask`
3. VyplÅˆ parametry:
   - `question`: "JakÃ¡ je doba plnÄ›nÃ­?"
   - `strict`: zaÅ¡krtni
   - `use_llm`: zaÅ¡krtni
4. Klikni **"Try it out"** â†’ vidÃ­Å¡ live odpovÄ›Ä

---

## Audit logging

VÅ¡echny dotazy jsou loggÃ¡ny do `logs/queries.jsonl`:

```json
{"timestamp": "2026-02-08T10:15:23.123456", "question": "JakÃ¡ je doba plnÄ›nÃ­?", "answer": "30 dnÃ­", "sources": [{"file": "smlouva.pdf", "page": 2, "chunk_id": 5}], "confidence": 0.92}
{"timestamp": "2026-02-08T10:16:45.654321", "question": "Kdo je objednatel?", "answer": "Firma ABC s.r.o.", "sources": [...], "confidence": 0.88}
```

Pro ÄtenÃ­ Log souborÅ¯:
```python
import json

with open("logs/queries.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        record = json.loads(line)
        print(f"Q: {record['question']}")
        print(f"A: {record['answer']}")
        print(f"Confidence: {record['confidence']}\n")
```

---

## TestovÃ¡nÃ­ Compliance Pravidel

SpusÅ¥ skript na ovÄ›Å™enÃ­, Å¾e RAG dodrÅ¾uje vÅ¡echna striktnÃ­ pravidla:

```bash
python test_compliance.py
```

**Kontroluje:**
- âœ… Hard factual gate (score < 0.72 â†’ fallback bez LLM)
- âœ… Keyword guard (relevantnÃ­ slova v kontextu)
- âœ… LLM jen jako kompresor (ne generÃ¡tor)
- âœ… Fallback konzistence

---

## Parametry a konfigurace

### Confidence threshold a Hard Factual Gate

```python
# V rag.py - Hard factual gate (POVINNÃ‰)
if top_score < 0.72:
    return {"answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech.", ...}
    # LLM se NESMÃ zavolat!
```

Pokud je nejrelevantnÄ›jÅ¡Ã­ chunk mÃ¡ cosine similarity < 0.72, vrÃ¡tÃ­ se fallback bez zavolÃ¡nÃ­ LLM. Toto je **kritickÃ© pravidlo** proti spekulacÃ­m.

### Chunk size a overlap

```python
# V build_index.py
engine.build_index(
    chunk_size=200,  # PoÄet slov v jednom chunku
    overlap=50       # PoÄet slov pÅ™ekryvu
)
```

VyÅ¡Å¡Ã­ overlap = lepÅ¡Ã­ seamless transition, ale je vÃ­ce chunkÅ¯.

### Strict mode

```python
# request
{"question": "...", "strict": true}
```

V strict mode se:
* pouÅ¾Ã­vÃ¡ niÅ¾Å¡Ã­ confidence threshold,
* odpovÄ›Ä se zkrÃ¡tÃ­ na max 2-3 vÄ›ty,
* vracÃ­ "NevÃ­m" mÃ­sto nejistÃ½ch odpovÄ›dÃ­.

---

## PoznÃ¡mky k nÃ¡vrhu

âœ… **BezpeÄnost:**
* Å½Ã¡dnÃ© scrapovÃ¡nÃ­ webu
* Å½Ã¡dnÃ¡ externÃ­ data
* Å½Ã¡dnÃ© halucinace
* PlnÄ› auditovatelnÃ© odpovÄ›di
* SYSTEM_PROMPT garantuje extrakci jen z kontextu

âœ… **VhodnÃ© pro:**
* Smlouvy a prÃ¡vnÃ­ dokumenty
* Compliance dokumentaci
* InternÃ­ dokumenty
* PoznÃ¡mky a pÅ™Ã­sluÅ¡nÃ©
* Q&A systÃ©my nad proprietary datou

âŒ **NevhodnÃ© pro:**
* Open-ended konverzace
* OtÃ¡zky vyÅ¾adujÃ­cÃ­ externÃ­ znalosti
* JazykovÃ© hry a humor
* TvoÅ™enÃ­ novÃ©ho obsahu (creative writing)

---

## Troubleshooting

### ProblÃ©m: "Index nenÃ­ dostupnÃ½"

```
HTTPException 503: Index nenÃ­ dostupnÃ½
```

**Å˜eÅ¡enÃ­:**
```bash
python build_index.py
```

MusÃ­Å¡ nejdÅ™Ã­v vytvoÅ™it index.

### ProblÃ©m: "ModuleNotFoundError: No module named 'openai'"

To je OK! Projekt fallbackuje na FLAN-T5. Pokud chceÅ¡ OpenAI:
```bash
pip install openai
export OPENAI_API_KEY="sk-..."
```

### ProblÃ©m: PomalÃ¡ odpovÄ›Ä

PravdÄ›podobnÄ› FLAN-T5 bÄ›Å¾Ã­ na CPU. Å˜eÅ¡enÃ­:
1. Instaluj OpenAI API (veel rychlejÅ¡Ã­)
2. Nebo pouÅ¾ij `use_llm=false` (extractive mode)

### ProblÃ©m: OdpovÄ›Ä je v angliÄtinÄ› (u Czech PDF)

SYSTEM_PROMPT Å™Ã­kÃ¡ "OdpovÃ­dej vÃ½hradnÄ› v ÄeskÃ©m jazyce". Pokud to nefunguje:
* OvÄ›Å™ Å¾e je SYSTEM_PROMPT korektnÄ› pÅ™edÃ¡n do LLM
* Zkus extractive mode (`use_llm=false`)
* Zktr ovÄ›Å™, Å¾e Python soubor mÃ¡ encoding `utf-8`

---

## Performance

| Faktor | Dopad |
|--------|-------|
| PDF velikost | MalÃ½ (chunking je offline) |
| PoÄet PDF | MalÃ½ (indexace je jedenkrÃ¡t) |
| Query latence (bez LLM) | **< 100ms** (FAISS je velmi rychlÃ½) |
| Query latence (s OpenAI) | **1-3s** (API latence) |
| Query latence (s FLAN-T5) | **2-5s** (CPU inference) |
| Embedding model size | ~460 MB (loaded once) |
| FAISS index size | ~1MB per 1000 vektory |

---

**Autor:** Adam Seifert  
**PoslednÃ­ aktualizace:** 2026-02-08  
**License:** MIT
