"""RAG engine: chunkovÃ¡nÃ­, embeddingy, index a retrieval v jednÃ© tÅ™Ã­dÄ›.

TÅ™Ã­da `RAGEngine` poskytuje metody pro vytvoÅ™enÃ­ indexu (`build_index`),
naÄtenÃ­ existujÃ­cÃ­ho indexu (`load_index`), vyhledÃ¡vÃ¡nÃ­ (`retrieve`) a
aplikaÄnÃ­ logiku odpovÃ­dÃ¡nÃ­ bez volÃ¡nÃ­ externÃ­ho LLM (`answer_question`).
"""

from pathlib import Path
import json
from typing import List, Dict, Any, Optional

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from rules import QUESTION_RULES


MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Heuristika pro detekci otÃ¡zek, kterÃ© oÄekÃ¡vajÃ­ krÃ¡tkou entitu jako odpovÄ›Ä
ENTITY_QUESTION_PREFIXES = (
    "kdo je",
    "jak se jmenuje",
    "kdo vystupuje",
    "kdo je uveden",
)


class RAGEngine:
    """KonsolidovanÃ½ RAG engine.

    - PouÅ¾Ã­vÃ¡ `sentence-transformers` pro embeddingy.
    - UklÃ¡dÃ¡ FAISS `IndexFlatIP` (normalizovanÃ© vektory -> kosinusovÃ¡ podobnost).
    - Metadata obsahujÃ­ `file`, `page`, `chunk_id`, `text`.
    """

    def __init__(self, model_name: str = MODEL_NAME):
        self.model = SentenceTransformer(model_name)
        self.index: Optional[faiss.Index] = None
        self.metadata: List[Dict[str, Any]] = []
        self.index_dir: Path = Path("index")

    def chunk_text(self, text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:
        """RozdÄ›lÃ­ dlouhÃ½ text na pÅ™ekrÃ½vajÃ­cÃ­ se chunky (sliding window).
        
        Princip:
        - RozdÄ›lÃ­me text NA SLOVA (ne znaky)
        - VytvoÅ™Ã­me okna o velikosti `chunk_size` slov
        - Posun okna je `step = chunk_size - overlap` slov
        - PÅ™ekryv umoÅ¾Åˆuje, aby se relevantnÃ­ fragment nevytratil na hranici
        
        PÅ™Ã­klad:
            text = "The quick brown fox jumps over..."
            chunk_size=5, overlap=2 -> step=3
            Chunk 1: "The quick brown fox jumps"
            Chunk 2: "fox jumps over the lazy"
            (vidÃ­me 'fox', 'jumps' v obou = seamless transition)
        
        Args:
            text: VstupnÃ­ text (obvykle jedna strÃ¡nka PDF)
            chunk_size: PoÄet slov v jednom chunku (default 200)
            overlap: PoÄet slov, kterÃ© se sdÃ­lÃ­ mezi sousednÃ­mi chunky (default 50)
        
        Returns:
            Seznam stringÅ¯, kaÅ¾dÃ½ string je jeden chunk (max chunk_size slov)
        """
        words = text.split()
        if not words:
            return []
        
        # Posun mezi chunky (o tento poÄet slov se posuneme dopÅ™edu)
        step = max(1, chunk_size - overlap)
        
        # Sliding window: vezmeme slova od i do i+chunk_size, posuneme o 'step'
        chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), step)]
        
        # Filtrace: ignorujeme prÃ¡zdnÃ© chunky
        return [c for c in chunks if c.strip()]

    def embed(self, texts: List[str]) -> np.ndarray:
        """VypoÄÃ­tÃ¡ embeddingy pomocÃ­ SentenceTransformer modelu.
        
        Model `paraphrase-multilingual-MiniLM-L12-v2`:
        - MnohajazyÄnÃ½ (podporuje 100+ jazykÅ¯ vÄetnÄ› ÄeÅ¡tiny)
        - KompaktnÃ­ (12 vrstev, 384 rozmÄ›rÅ¯)
        - Fast (CPU efficient)
        - Trained na semantickÃ© podobnosti
        
        Embedding:
        - Vstup: seznam textÅ¯
        - VÃ½stup: matice formÃ¡tu (N, 384), kde N = poÄet textÅ¯
        - Dtype: float32 (kompatibilita s FAISS)
        
        Args:
            texts: Seznam stringÅ¯ k enkÃ³dovÃ¡nÃ­
        
        Returns:
            np.ndarray formÃ¡tu (len(texts), 384) typu float32
        """
        vecs = self.model.encode(texts, show_progress_bar=False)
        return np.array(vecs, dtype=np.float32)

    def build_index(self, data_dir: str = "data", index_dir: str = "index", chunk_size: int = 200, overlap: int = 50, batch_size: int = 64):
        """VytvoÅ™Ã­ FAISS index z PDF souborÅ¯: PDF â†’ Chunky â†’ Embeddingy â†’ Index.
        
        Procedura:
        1. Najde vÅ¡echny *.pdf soubory v `data_dir`
        2. Pro kaÅ¾dÃ½ PDF:
           - Extrahuje text strÃ¡nku po strÃ¡nce (pdf_loader.load_pdf)
           - RozdÄ›lÃ­ text na chunky se overlappem (chunk_text)
           - SchovÃ¡ metadata (soubor, strÃ¡nka, chunk_id)
        3. VypoÄÃ­tÃ¡ embeddingy pro vÅ¡echny chunky (po dÃ¡vkÃ¡ch)
        4. Normalizuje embeddingy (kritickÃ© pro IndexFlatIP)
        5. VytvoÅ™Ã­ FAISS IndexFlatIP a uloÅ¾Ã­ na disk:
           - index/faiss.index (vektorovÃ½ index)
           - index/documents.json (metadata)
        
        VÃ½stup struktura:
            index/
            â”œâ”€â”€ faiss.index               # FAISS IndexFlatIP (binÃ¡rnÃ­)
            â””â”€â”€ documents.json            # Metadata JSON
                [
                  {
                    "file": "smlouva.pdf",
                    "page": 1,
                    "chunk_id": 1,
                    "text": "prvnÃ­ch 200 slov strÃ¡nky 1..."
                  },
                  ...
                ]
        
        Args:
            data_dir: AdresÃ¡Å™ s PDF soubory (default "data/")
            index_dir: VÃ½stupnÃ­ adresÃ¡Å™ pro index (default "index/")
            chunk_size: PoÄet slov v jednom chunku (default 200)
            overlap: PoÄet slov pÅ™ekryvu mezi chunky (default 50)
            batch_size: PoÄet textÅ¯ zpracovanÃ½ch najednou (default 64, pro rychlost)
        """
        from pdf_loader import load_pdf

        data_dir = Path(data_dir)
        index_dir = Path(index_dir)
        index_dir.mkdir(parents=True, exist_ok=True)

        # Krok 1: SbÃ­rÃ¡nÃ­ chunkÅ¯
        chunks = []

        # Najdi vÅ¡echny PDF soubory
        for pdf_file in data_dir.glob("*.pdf"):
            print(f"ğŸ“„ ÄŒtenÃ­: {pdf_file.name}")
            
            # Extrahuj text (strÃ¡nku po strÃ¡nce)
            pages = load_pdf(str(pdf_file))
            
            for page in pages:
                page_text = page.get("text", "")
                
                # RozdÄ›lenÃ­ strÃ¡nky na chunky
                page_chunks = self.chunk_text(page_text, chunk_size=chunk_size, overlap=overlap)
                
                for cid, chunk in enumerate(page_chunks, start=1):
                    chunks.append({
                        "file": pdf_file.name,
                        "page": page.get("page"),
                        "chunk_id": cid,
                        "text": chunk,
                    })

        if not chunks:
            print("âš  Å½Ã¡dnÃ© textovÃ© chunky k indexovÃ¡nÃ­. Zkontroluj data/ adresÃ¡Å™.")
            return

        print(f"âœ“ SbÃ­rÃ¡no {len(chunks)} chunkÅ¯")

        # Krok 2: EnkÃ³dovÃ¡nÃ­ (po dÃ¡vkÃ¡ch)
        texts = [c['text'] for c in chunks]
        all_embs = []
        
        print(f"ğŸ”„ EnkÃ³dovÃ¡nÃ­ {len(texts)} textÅ¯ po dÃ¡vkÃ¡ch x{batch_size}...")
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            emb = self.embed(batch)
            all_embs.append(emb)
        
        embeddings = np.vstack(all_embs).astype(np.float32)
        print(f"âœ“ Embeddingy shape: {embeddings.shape}")

        # Krok 3: Normalizace (KRITICKÃ‰ pro IndexFlatIP)
        # IndexFlatIP oÄekÃ¡vÃ¡ normalizovanÃ© vektory (jednotkovÃ½ norm)
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        embeddings = embeddings / norms
        print(f"âœ“ Vektory normalizovÃ¡ny")

        # Krok 4: VytvoÅ™enÃ­ a uloÅ¾enÃ­ FAISS indexu
        dim = embeddings.shape[1]  # 384 pro paraphrase-multilingual-MiniLM
        index = faiss.IndexFlatIP(dim)
        index.add(embeddings)
        print(f"âœ“ FAISS IndexFlatIP vytvoÅ™en (dim={dim}, n={len(embeddings)})")

        # UloÅ¾i index a metadata
        faiss.write_index(index, str(index_dir / "faiss.index"))
        with open(index_dir / "documents.json", 'w', encoding='utf-8') as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)

        print(f"âœ… Index uloÅ¾en: {index_dir / 'faiss.index'}")
        print(f"âœ… Metadata uloÅ¾ena: {index_dir / 'documents.json'}")

    def load_index(self, index_dir: str = "index"):
        """NaÄte existujÃ­cÃ­ FAISS index a metadata ze souboru.
        
        PÅ™edpoklady:
        - `index_dir/faiss.index` existuje (binÃ¡rnÃ­ FAISS index)
        - `index_dir/documents.json` existuje (metadata)
        
        PouÅ¾itÃ­:
            engine = RAGEngine()
            engine.load_index('index/')  # NaÄte index
            results = engine.retrieve("JakÃ¡ je doba plnÄ›nÃ­?")  # NynÃ­ funguje
        
        Args:
            index_dir: AdresÃ¡Å™ s uloÅ¾enÃ½m indexem (default "index/")
        
        Raises:
            FileNotFoundError: Pokud index nebo metadata chybÃ­
        """
        idx_path = Path(index_dir) / "faiss.index"
        meta_path = Path(index_dir) / "documents.json"
        
        # Kontrola: jsou soubory pÅ™Ã­tomnÃ©?
        if not idx_path.exists() or not meta_path.exists():
            raise FileNotFoundError(
                f"Index nebo metadata chybÃ­ v '{index_dir}'. "
                "SpusÅ¥te: python build_index.py"
            )

        # NaÄti FAISS index (binÃ¡rnÃ­ formÃ¡t)
        self.index = faiss.read_index(str(idx_path))
        
        # NaÄti metadata (JSON)
        with open(meta_path, 'r', encoding='utf-8') as f:
            self.metadata = json.load(f)
        
        self.index_dir = Path(index_dir)
        print(f"âœ“ Index naÄten: {len(self.metadata)} dokumentÅ¯, "
              f"vimenze={self.index.d}")

    def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """VrÃ¡tÃ­ top-k nejrelevantnÄ›jÅ¡Ã­ch chunkÅ¯ z FAISS indexu (semantic search).
        
        Algoritmus:
        1. EnkÃ³duj query pomocÃ­ SentenceTransformer (stejnÃ½ model jako chunky)
        2. Normalizuj query embedding (potÅ™ebnÃ© pro IndexFlatIP)
        3. Hledej k-nearest neighbors v FAISS indexu (kosinusovÃ¡ podobnost)
        4. VraÅ¥ vÃ½sledky s skÃ³rem (0.0-1.0) a metadaty
        
        FAISS IndexFlatIP:
        - "IP" znamenÃ¡ Inner Product (vnitÅ™nÃ­ souÄin)
        - Pokud jsou vektory normalizovÃ¡ny na jednotkou dÃ©lku,
          Inner Product = KosinusovÃ¡ Podobnost
        - VracÃ­ "distances" jako cosine similarities
        
        PÅ™Ã­klad vÃ½stupu:
        [
            {
                "score": 0.87,
                "file": "smlouva.pdf",
                "page": 2,
                "chunk_id": 5,
                "text": "Doba plnÄ›nÃ­ je 30 dnÃ­ od data..."
            },
            {
                "score": 0.73,
                "file": "smlouva.pdf",
                "page": 3,
                "chunk_id": 7,
                "text": "PlnÄ›nÃ­ se musÃ­ uskuteÄnit..."
            },
            ...
        ]
        
        Args:
            query: OtÃ¡zka nebo vyhledÃ¡vacÃ­ vÃ½raz (string)
            k: PoÄet nejlepÅ¡Ã­ch vÃ½sledkÅ¯ (default 5)
        
        Returns:
            Seznam dict se score (0.0-1.0), file, page, chunk_id, text
            SeÅ™azeno sestupnÄ› podle score (nejvyÅ¡Å¡Ã­ skÃ³re prvnÃ­)
        """
        if self.index is None or not self.metadata:
            return []

        # EnkÃ³duj query
        q_emb = self.embed([query]).astype(np.float32)
        
        # Normalizuj query (FAISS IndexFlatIP vyÅ¾aduje normalizovanÃ© vektory)
        q_norm = np.linalg.norm(q_emb, axis=1, keepdims=True)
        q_norm[q_norm == 0] = 1.0  # OÅ¡etÅ™enÃ­ dÄ›lenÃ­ nulou
        q_emb = q_emb / q_norm

        # FAISS semantic search (vrÃ¡tÃ­ distances = cosine similarities)
        distances, indices = self.index.search(q_emb, k)

        # Konstruuj vÃ½sledky s metadaty
        results = []
        for score, idx in zip(distances[0], indices[0]):
            if idx < 0 or idx >= len(self.metadata):
                continue  # Index mimo rozsah (FAISS vracÃ­ -1 pro neplatnÃ©)
            # SpojÃ­me metadata s skÃ³rem
            results.append({"score": float(score), **self.metadata[idx]})

        # SeÅ™aÄ sestupnÄ› podle score (nejlepÅ¡Ã­ prvnÃ­)
        results = sorted(results, key=lambda x: x['score'], reverse=True)
        return results

    def answer_question(self, question: str, retrieved: List[Dict[str, Any]], strict: bool = False, threshold: float = 0.45, top_k_texts: int = 3) -> Dict[str, Any]:
        """OdpovÃ­dÃ¡ na otÃ¡zku pomocÃ­ pravidel (rules.py) - **BEZ LLM**.
        
        Aplikuje tvrdÃ¡ pravidla definovanÃ¡ v QUESTION_RULES:
        1. Detekuje typ otÃ¡zky (doba_plneni, platnost_smlouvy, objednatel atd.)
        2. Filtruje chunky podle min_similarity prahu a section_keywords
        3. VracÃ­ nejlepÅ¡Ã­ chunk nebo fallback zprÃ¡vu
        
        Args:
            question: UÅ¾ivatelskÃ¡ otÃ¡zka
            retrieved: Ignoruje se (kompatibilita), vnitÅ™nÄ› volÃ¡ self.retrieve()
            strict: NepouÅ¾Ã­vÃ¡ se (kompatibilita)
            threshold: NepouÅ¾Ã­vÃ¡ se (min_similarity je v QUESTION_RULES)
            top_k_texts: PoÄet chunkÅ¯ k naÄtenÃ­
        
        Returns:
            {"answer": str, "sources": List[Dict], "confidence": float}
        """
        # Detekce typu otÃ¡zky (napÅ™. "doba plnÄ›nÃ­" -> "doba_plneni")
        qtype = detect_question_type(question)

        if qtype is None:
            # OtÃ¡zka neodpovÃ­dÃ¡ Å¾Ã¡dnÃ©mu pravidlu
            return {"answer": "NevÃ­m.", "sources": [], "confidence": 0.0}

        # NaÄteme pravidla pro danÃ½ typ (min_similarity, keywords atd.)
        rules = QUESTION_RULES[qtype]

        # Semantic search v FAISS indexu
        results = self.retrieve(question, k=top_k_texts)

        # Filtrujeme podle pravidel (kosinusovÃ¡ podobnost + section keywords)
        allowed = []
        for r in results:
            if (
                r.get("score", 0.0) >= rules.get("min_similarity", 0.0)
                and is_chunk_allowed(r.get("text", ""), rules)
            ):
                allowed.append(r)

        # Å½Ã¡dnÃ½ chunk neproÅ¡el kombinovanÃ½m filtrem
        if not allowed:
            return {
                "answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech explicitnÄ› uvedena.",
                "sources": [],
                "confidence": 0.0,
            }

        # Vezmeme nejlepÅ¡Ã­ chunk (nejvyÅ¡Å¡Ã­ skÃ³re)
        best = sorted(allowed, key=lambda x: x.get("score", 0.0), reverse=True)[0]

        return {
            "answer": best.get("text", "").strip(),
            "sources": [{
                "file": best.get("file"),
                "page": best.get("page"),
                "chunk_id": best.get("chunk_id"),
            }],
            "confidence": round(best.get("score", 0.0), 3),
        }

    def synthesize_answer(self, question: str, retrieved: List[Dict[str, Any]], use_llm: bool = True, strict: bool = False) -> Dict[str, Any]:
        """Syntetizuje odpovÄ›Ä se striktnÃ½mi strÃ¡Å¾ci protiv halucinacÃ­.
        
        **KritickÃ© pravidla (compliance-friendly):**
        
        1. **Hard factual gate** (score < 0.72)
           - Pokud top chunk mÃ¡ skÃ³re < 0.72, vraÅ¥ fallback BEZ LLM
           - ZabrÃ¡nÃ­ LLM aby spekuloval
        
        2. **Keyword guard**
           - OvÄ›Å™ Å¾e kontext obsahuje relevantnÃ­ slova z otÃ¡zky
           - Pokud chybÃ­ (napÅ™. "vlastnÃ­k" nenÃ­ v textu), vraÅ¥ fallback
        
        3. **LLM jen jako kompresor**
           - LLM dostane pouze extrahovanÃ© texty
           - Never generuje novÃ© informace
        
        Pipeline:
        1. Check: MÃ¡me vÅ¯bec chunky?
        2. CHECK: top_score >= 0.72? (Hard gate)
        3. CHECK: Obsahuje relevantnÃ­ keywords? (Guard)
        4. Pokud use_llm: zavolej LLM jako KOMPRESOR (ne syntezÃ¡tor)
        5. Pokud strict: zkrÃ¡tÃ­ na max 2-3 vÄ›ty
        
        Args:
            question: UÅ¾ivatelskÃ¡ otÃ¡zka
            retrieved: Top-k chunky z FAISS retrieve()
            use_llm: PouÅ¾Ã­t LLM jako kompresor (True) nebo vrÃ¡tit raw text (False)
            strict: Pokud True, zkrÃ¡tÃ­ odpovÄ›Ä na 2-3 vÄ›ty
        
        Returns:
            {"answer": str, "sources": List[Dict], "confidence": float}
            
            Note: answer = "" pokud neproÅ¡la gate/guard (fallback)
        """
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 1ï¸âƒ£ CHECK: MÃ¡me vÅ¯bec relevantnÃ­ chunky?
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not retrieved:
            return {
                "answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech.",
                "sources": [],
                "confidence": 0.0,
            }

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2ï¸âƒ£ HARD FACTUAL GATE: top_score >= 0.50?
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        top_score = retrieved[0].get("score", 0.0)
        HARD_GATE_THRESHOLD = 0.50  # SnÃ­Å¾eno pro lepÅ¡Ã­ recall, odpovÃ­dÃ¡ compliance testÅ¯m
        
        if top_score < HARD_GATE_THRESHOLD:
            # âŒ SkÃ³re pÅ™Ã­liÅ¡ nÃ­zkÃ© = informace nenÃ­ dostateÄnÄ› podloÅ¾enÃ¡
            # LLM se NESMÃ zavolat (bez spekulace)
            return {
                "answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech.",
                "sources": [],
                "confidence": top_score,
            }

        # SpojenÃ­ textu ze vÅ¡ech chunkÅ¯
        context = "\n---\n".join([r.get("text", "") for r in retrieved])
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3ï¸âƒ£ KEYWORD GUARD: Obsahuje relevantnÃ­ slova?
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not self._has_relevant_keywords(context, question):
            # âŒ Kontext nemÃ¡ relevantnÃ­ slova = otÃ¡zka je mimo scope
            return {
                "answer": "PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech.",
                "sources": [],
                "confidence": top_score,
            }

        # PrÅ¯mÄ›rnÃ© skÃ³re relevance
        avg_score = sum(r.get("score", 0.0) for r in retrieved) / len(retrieved) if retrieved else 0.0

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 4ï¸âƒ£ VOLITELNÃ‰: LLM jako KOMPRESOR (ne syntezÃ¡tor)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if use_llm:
            try:
                from llm import LLMWrapper
                llm = LLMWrapper(use_openai=True)
                # âš ï¸ NOVÃ: compress_answer mÃ­sto synthesize
                # (jen zkrÃ¡tÃ­, ne generuje)
                answer_text = llm.compress_answer(question, context)
            except Exception as e:
                # Fallback: vraÅ¥ raw kontext (bez LLM)
                print(f"âš  LLM chyba: {e}, fallback na raw kontext")
                answer_text = context
        else:
            # No LLM: vraÅ¥ raw context
            answer_text = context

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 5ï¸âƒ£ POKUD strict: zkrÃ¡tÃ­me na 2-3 vÄ›ty
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if strict:
            answer_text = self._summarize_answer(answer_text, max_sentences=3)

        return {
            "answer": answer_text.strip(),
            "sources": [
                {
                    "file": r.get("file"),
                    "page": r.get("page"),
                    "chunk_id": r.get("chunk_id"),
                }
                for r in retrieved
            ],
            "confidence": round(avg_score, 3),
        }

    def _has_relevant_keywords(self, context: str, question: str) -> bool:
        """OvÄ›Å™, Å¾e kontext obsahuje relevantnÃ­ klÃ­ÄovÃ¡ slova z otÃ¡zky.
        
        ÃšÄŒEL: ZabrÃ¡nit LLM aby spekuloval na otÃ¡zky mimo scope dokumentu.
        
        PÅ™Ã­klady sprÃ¡vnÃ©ho chovÃ¡nÃ­:
        - Q: "JakÃ¡ je doba plnÄ›nÃ­?"           K: ["doba", "plnÄ›nÃ­"] â†’ True (slova v textu)
        - Q: "Kdo je objednatel?"             K: ["objednatel"] â†’ True
        - Q: "Kdo je skuteÄnÃ½ vlastnÃ­k?"      K: [] (ve smlouvÄ› nenÃ­) â†’ False (vraÅ¥ fallback)
        - Q: "JakÃ© riziko smlouva pÅ™edstavuje?" K: [] (evaluaÄnÃ­, ne faktickÃ¡) â†’ False
        - Q: "JakÃ¡ je cena na trhu?"          K: [] (externÃ­ data) â†’ False
        
        ALGORITMUS:
        1. Extrahuj keywords z otÃ¡zky (slova delÅ¡Ã­ neÅ¾ 3 znaky, mimo stop-words)
        2. Normalizuj context na lowercase
        3. SpoÄÃ­tej kolik keywords je pÅ™Ã­tomno v contextu
        4. Pokud mÃ©nÄ› neÅ¾ 2 keywords â†’ vraÅ¥ False (fallback)
        5. Pokud alespoÅˆ 2 â†’ vraÅ¥ True (proceed)
        
        Args:
            context: ExtrahovanÃ© texty z FAISS chunkÅ¯
            question: UÅ¾ivatelskÃ¡ otÃ¡zka
        
        Returns:
            True pokud je dostateÄnÄ› relevantnÃ­, False â†’ fallback
        """
        import string
        
        # Normalizuj
        q_lower = question.lower()
        c_lower = context.lower()
        
        # Stop-words: ignoruj tyto slova (jsou pÅ™Ã­liÅ¡ generickÃ¡)
        stop_words = {
            'jakÃ¡', 'jakÃ©', 'jakÃ½', 'je', 'co', 'za', 'byl', 'byla', 'bylo',
            'jsou', 'budou', 'pokud', 'pokud', 'nebo', 'a', 'z', 'na',
            'ten', 'ta', 'to', 'ten', 'tou', 'tÃ­m', 'jakÃ½m', 'kterou', 'kterÃ½',
            'si', 'se', 'i', 'o', 'do', 'by', 'by', 'by'
        }
        
        # Extrahuj keywords z otÃ¡zky (slova delÅ¡Ã­ neÅ¾ 3 znaky, bez interpunkce)
        q_words = [
            w.strip().strip(string.punctuation)  # OdstraÅˆ interpunkci
            for w in q_lower.split()
            if len(w.strip().strip(string.punctuation)) > 3 and w.strip().strip(string.punctuation) not in stop_words
        ]
        
        # Pokud otÃ¡zka nemÃ¡ Å¾Ã¡dnÃ¡ keywords (nemÄ›l by se stÃ¡t), povolj
        if not q_words:
            return True
        
        # Hledej kolik keywords je pÅ™Ã­tomno v contextu
        matched = sum(1 for w in q_words if w in c_lower)
        
        # PovinnÃ¡ pravidla:
        # - AlespoÅˆ 2 keywords musÃ­ bÃ½t v contextu
        # - Nebo alespoÅˆ 60% keywords
        min_match = max(2, len(q_words) // 2)  # AlespoÅˆ 2 nebo 50%
        
        return matched >= min_match

    def _summarize_answer(self, text: str, max_sentences: int = 3) -> str:
        """ZkrÃ¡tÃ­ odpovÄ›Ä na max_sentences vÄ›t pomocÃ­ heuristiky (bez LLM).
        
        PouÅ¾Ã­vÃ¡ se v strict mode aktivovÃ¡n synthesize_answer(..., strict=True).
        
        Algoritmus:
        1. RozdÄ›lÃ­ text na vÄ›ty regex: za teÄkou/vÃ½kÅ™/otaznÃ­kem + whitespace
        2. Vezme prvnÃ­ch max_sentences vÄ›t
        3. ZajistÃ­, Å¾e vÃ½sledek konÄÃ­ sprÃ¡vnou interpunkcÃ­
        
        VÃ½hody:
        - Bez LLM = deterministickÃ©, bez halucinacÃ­
        - ZachovÃ¡vÃ¡ poÄÃ¡teÄnÃ­ relevantnÃ­ informace
        - SplÅˆuje poÅ¾adavek "max 2-3 vÄ›ty pro asistenty"
        
        PÅ™Ã­klad:
            text = "Doba plnÄ›nÃ­ je 30 dnÃ­. To je standard. Lze ji prodlouÅ¾it."
            max_sentences = 2
            VÃ½stup: "Doba plnÄ›nÃ­ je 30 dnÃ­. To je standard."
        
        Args:
            text: CelÃ½ text odpovÄ›di
            max_sentences: MaximÃ¡lnÃ­ poÄet vÄ›t v output (default 3)
        
        Returns:
            ZkrÃ¡cenÃ½ text (max max_sentences vÄ›t) s sprÃ¡vnou interpunkcÃ­
        """
        import re
        
        # RozdÄ›lenÃ­ na vÄ›ty: hledej zbytky za [.!?] a nÃ¡sledujÃ­cÃ­m whitespace
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        
        # OÄisti: seÅ™aÄ, a odstraÅˆ prÃ¡zdnÃ© vÄ›ty
        sentences = [s.strip() for s in sentences if s.strip()]

        # Pokud je vÄ›t mÃ©nÄ› neÅ¾ max_sentences, vraÅ¥ celÃ½ text
        if len(sentences) <= max_sentences:
            return text.strip()

        # Vezmi prvnÃ­ch max_sentences vÄ›t
        summary = " ".join(sentences[:max_sentences])
        
        # Ujisti se, Å¾e text konÄÃ­ nÄ›jakou interpunkcÃ­
        if summary and not summary.endswith(('.', '!', '?')):
            summary += "."
        
        return summary

    def is_entity_question(self, question: str) -> bool:
        """RozpoznÃ¡ jednoduchÃ© entity-type otÃ¡zky podle pÅ™eddefinovanÃ½ch prefixÅ¯.
        
        Entity otÃ¡zky: ty, kterÃ© oÄekÃ¡vajÃ­ menÅ¡Ã­ kus informace (osobu, firmu, apod)
        PÅ™Ã­klady: "Kdo je objednatel?", "Jak se jmenuje dodavatel?", ...
        
        HledÃ¡ otÃ¡zky zaÄÃ­najÃ­cÃ­ se slovy: "kdo je", "jak se jmenuje", "kdo vystupuje", ...
        
        PouÅ¾Ã­vÃ¡ se pro optimalizaci extrakce odpovÄ›di:
        - Pokud je entity question, mÅ¯Å¾eme hledat konkrÃ©tnÃ­ vÄ›tu mÃ­sto dlouhÃ©ho textu
        
        Args:
            question: UÅ¾ivatelskÃ¡ otÃ¡zka (string)
        
        Returns:
            True pokud otÃ¡zka je entity-type, else False
        """
        q = (question or "").lower().strip()
        return any(q.startswith(p) for p in ENTITY_QUESTION_PREFIXES)

    def _extract_entity_sentence(self, text: str, keyword: Optional[str] = None) -> str:
        """Extrahuje nejrelevantnÄ›jÅ¡Ã­ vÄ›tu z textu (pro entity otÃ¡zky).
        
        Strategie:
        1. Pokud je keyword poskytnut: najdi PRVNÃ vÄ›tu obsahujÃ­cÃ­ keyword
        2. Pokud nenÃ­ keyword (nebo nenÃ­ v textu): vraÅ¥ PRVNÃ vÄ›tu
        
        PouÅ¾Ã­vÃ¡ se v answer_question() pro entity otÃ¡zky ("Kdo je X?")
        
        PÅ™Ã­klad:
            text = "Pan NovÃ¡k je objednatel. Podpisem smlouvy souhlasÃ­. ..."
            keyword = "objednatel"
            VÃ½stup: "Pan NovÃ¡k je objednatel."
        
        Args:
            text: VstupnÃ­ text (obvykle jeden chunk)
            keyword: Slovo k vyhledÃ¡nÃ­ v jednÃ© z vÄ›t (optional)
        
        Returns:
            VÄ›ta obsahujÃ­cÃ­ keyword (pokud keyword je), nebo prvnÃ­ vÄ›ta
        """
        import re
        
        if not text:
            return ""
        
        # RozdÄ›lenÃ­ textu na vÄ›ty
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        sentences = [s.strip() for s in sentences if s.strip()]

        # Pokud mÃ¡me keyword, hledej vÄ›tu s tÃ­mto slovem
        if keyword:
            k = keyword.lower()
            for sentence in sentences:
                if k in sentence.lower():
                    return sentence

        # Fallback: vraÅ¥ prvnÃ­ vÄ›tu (pokud existuje)
        return sentences[0] if sentences else text.strip()


def detect_question_type(question: str) -> str | None:
    """Detekuje typ otÃ¡zky porovnÃ¡nÃ­m against QUESTION_RULES z rules.py.
    
    Algoritmus:
    1. Normalizuj otÃ¡zku na malÃ¡ pÃ­smena
    2. Pro kaÅ¾dÃ½ rule_type v QUESTION_RULES:
       - Hledej alespoÅˆ jedno question_keyword v otÃ¡zce
       - Pokud najdeÅ¡, vraÅ¥ rule_type
    3. Pokud nic nepaduje, vraÅ¥ None
    
    PÅ™Ã­klady:
        "JakÃ¡ je doba plnÄ›nÃ­?" -> "doba_plneni" (najde "doba plnÄ›nÃ­")
        "Kolik je platnost smlouvy?" -> "platnost_smlouvy"
        "Kdo je vyrobce?" -> None (nenÃ­ v QUESTION_RULES)
    
    Args:
        question: UÅ¾ivatelskÃ¡ otÃ¡zka (string)
    
    Returns:
        KlÃ­Ä z QUESTION_RULES (str) nebo None pokud neodpovÃ­dÃ¡ Å¾Ã¡dnÃ©mu pravidlu
    """
    q = (question or "").lower()
    
    # Iteruj pÅ™es vÅ¡echna dostupnÃ¡ pravidla
    for qtype, cfg in QUESTION_RULES.items():
        # Zkontroluj, zda otÃ¡zka obsahuje alespoÅˆ jedno question_keyword
        for kw in cfg.get("question_keywords", []):
            if kw in q:
                return qtype  # NaÅ¡li jsme shodu, vraÅ¥ typ
    
    return None  # Å½Ã¡dnÃ¡ shoda


def is_chunk_allowed(chunk_text: str, rules: dict) -> bool:
    """TvrdÃ¡ filtrace: kontroluj, zda chunk obsahuje section_keywords z pravidel.
    
    PouÅ¾Ã­vÃ¡ se v answer_question() pro dalÅ¡Ã­ filtraci retrievanÃ½ch chunkÅ¯.
    
    Algoritmus:
    1. Normalizuj chunk na malÃ¡ pÃ­smena
    2. Hledej alespoÅˆ jedno section_keyword ze `rules` v text
    3. Pokud ano, vraÅ¥ True (chunk "projde")
    4. Pokud ne, vraÅ¥ False (ignoruj chunk)
    
    PÅ™Ã­klad:
        chunk_text = "Doba plnÄ›nÃ­ je 30 dnÃ­ od podpisu smlouvy"
        rules = QUESTION_RULES["doba_plneni"]
                = {
                    "question_keywords": [...],
                    "section_keywords": ["doba plnÄ›nÃ­", "plnÄ›nÃ­ smlouvy", ...],
                    "min_similarity": 0.75
                  }
        is_chunk_allowed(...) -> True (obsahuje "doba plnÄ›nÃ­")
    
    Args:
        chunk_text: Text chunku z FAISS indexu
        rules: Dictionary s section_keywords (z QUESTION_RULES)
    
    Returns:
        True pokud chunk obsahuje alespoÅˆ jedno section_keyword, else False
    """
    text = (chunk_text or "").lower()
    
    # Zkontroluj vÅ¡echny section keywords
    return any(kw in text for kw in rules.get("section_keywords", []))


if __name__ == '__main__':
    print('This module provides RAGEngine. Use build_index.py to create an index.')
