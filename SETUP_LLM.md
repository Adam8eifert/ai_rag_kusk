# LLM Integration: KompletnÃ­ prÅ¯vodce

## Diagram: Jak to funguje

```
Dotaz â†’ FAISS retrieve â†’ LLM syntetizace â†’ REST API response
                     â†“
              (kontext)
            top-k chunky
         se systÃ©m promptem
```

## Konfigurace: OpenAI vs Local

### ğŸŸ¢ Varianta 1: OpenAI API (doporuÄeno)

```bash
# 1. Instalace (je v requirements.txt)
pip install openai>=1.0.0

# 2. NastavenÃ­ API key
export OPENAI_API_KEY="sk-..."

# 3. Test
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"JakÃ¡ je doba plnÄ›nÃ­?", "use_llm": true}'
```

**VÃ½hody:**
- VysokÃ¡ kvalita odpovÄ›dÃ­
- Podpora ÄeÅ¡tiny
- BezpeÄnost (bez staÅ¾enÃ­ velkÃ©ho modelu)

**NevÃ½hody:**
- PlacenÃ© (0.0005 USD per 1K tokens)
- ZÃ¡vislost na OpenAI

### ğŸŸ¡ Varianta 2: Local FLAN-T5 (fallback)

Automaticky se pouÅ¾Ã­vÃ¡, pokud:
- `OPENAI_API_KEY` nenÃ­ nastavenÃ½
- `openai` nenÃ­ nainstalovanÃ½

**VÃ½hody:**
- Zdarma
- Offline
- Bez latence

**NevÃ½hody:**
- Kvalita odpovÄ›dÃ­ je niÅ¾Å¡Ã­
- VyÅ¾aduje ~2GB VRAM
- Generace mÅ¯Å¾e bÃ½t pomalÃ¡

---

## SpuÅ¡tÄ›nÃ­ serveru

### Development (local FLAN-T5):
```bash
cd /home/adam/Dokumenty/projects/ai_rag_kusk
source .venv/bin/activate
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

### Production (OpenAI):
```bash
export OPENAI_API_KEY="sk-..."
uvicorn app:app --host 0.0.0.0 --port 8000
```

---

## API Endpoint

### POST `/ask`

**Request:**
```json
{
  "question": "JakÃ¡ je doba plnÄ›nÃ­?",
  "k": 5,
  "use_llm": true
}
```

**Response:**
```json
{
  "answer": "Doba plnÄ›nÃ­ je 30 dnÅ¯ od podpisu smlouvy.",
  "sources": [
    {
      "file": "smlouva.pdf",
      "page": 3,
      "chunk_id": 1
    }
  ],
  "confidence": 0.785
}
```

**ChovÃ¡nÃ­:**
- `use_llm=true`: LLM syntetizuje odpovÄ›Ä (kvalita)
- `use_llm=false`: VrÃ¡tÃ­ syrovÃ© chunky (rychleji, bez syntÃ©zy)
- `k`: PoÄet retrievenÃ½ch chunkÅ¯ (default: 5)

---

## Prompts

### SystÃ©m:
```
Jsi extrakÄnÃ­ asistent. OdpovÃ­dej pouze na zÃ¡kladÄ› poskytnutÃ©ho kontextu. 
Pokud odpovÄ›Ä v kontextu nenÃ­, Å™ekni pÅ™esnÄ›: 
'PoÅ¾adovanÃ¡ informace nenÃ­ v dokumentech explicitnÄ› uvedena.'
OdpovÄ›z jednou nebo dvÄ›ma vÄ›tami bez parafrÃ¡zÃ­ prÃ¡vnÃ­ho obsahu.
```

### User:
```
KONTEXT:
{retrieved_chunks_joined}

OTÃZKA:
{question}

OdpovÄ›z:
```

---

## TestovÃ¡nÃ­

```bash
# Test script
python test_llm_synthesis.py

# cURL
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Kdo je objednatelem smlouvy?",
    "k": 5,
    "use_llm": true
  }'
```

---

## Troubleshooting

| ProblÃ©m | Å˜eÅ¡enÃ­ |
|---------|--------|
| `ModuleNotFoundError: openai` | `pip install openai>=1.0.0` |
| OPENAI_API_KEY error | `export OPENAI_API_KEY="sk-..."` |
| PomalÃ¡ odpovÄ›Ä | SniÅ¾te `k` parametr, nebo pouÅ¾ijte `use_llm=false` |
| FLAN-T5 VRAM error | Redeploy s OpenAI API |
| OdpovÄ›Ä je v angliÄtinÄ› | Testujete s FLAN-T5; pÅ™epnÄ›te na OpenAI |

---

## Architektura

```
app.py (endpoint /ask)
  â””â”€> RAGEngine.retrieve()           # FAISS
  â””â”€> RAGEngine.synthesize_answer() # LLM wrapper
       â””â”€> LLMWrapper.synthesize()
            â”œâ”€> OpenAI API (priority)
            â””â”€> Local FLAN-T5 (fallback)
```

---

## Performance

| LLM | Latence | Kvalita | NÃ¡klady |
|-----|---------|---------|---------|
| OpenAI GPT-3.5 | ~500ms | â˜…â˜…â˜…â˜…â˜… | 0.0005 USD/req |
| FLAN-T5 local | ~2s | â˜…â˜…â˜† | 0 USD |

---

## BezpeÄnost

- **Context injection**: LLM je instrukovÃ¡n, aby ignoroval mimo-kontext
- **Token limits**: Max 200 tokens na odpovÄ›Ä
- **Fallback**: Pokud LLM selÅ¾e, vracÃ­ se syrÃ½ kontext
